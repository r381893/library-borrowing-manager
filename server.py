"""
åœ–æ›¸é¤¨å€Ÿæ›¸ç®¡ç†ç³»çµ± - Python å¾Œç«¯ API
ç›´æ¥è®€å¯« Excel æª”æ¡ˆï¼Œæä¾› RESTful API çµ¦å‰ç«¯ä½¿ç”¨
"""

from flask import Flask, jsonify, request, send_file
from flask_cors import CORS
import pandas as pd
import os
from datetime import datetime
import logging
import traceback

# è¨­å®š Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("server.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # å…è¨±è·¨åŸŸè«‹æ±‚

# Excel æª”æ¡ˆè·¯å¾‘
EXCEL_FILE = os.path.join(os.path.dirname(__file__), 'åœ–æ›¸é¤¨å€Ÿæ›¸æ¸…å–®.xlsx')
LAST_MTIME = 0
CACHED_BOOKS = None

# åˆ†é¡å°æ‡‰çš„å·¥ä½œè¡¨åç¨±
CATEGORIES = [
    'æ–°æ›¸-å¾…å€Ÿ',
    'å¾…å€Ÿ',
    'ä¸èƒ½å€Ÿ',
    'é£Ÿè­œ',
    'é æ•¸å¤ªå¤š',
    'å·²çœ‹-3447æœ¬',
    'å·²çœ‹-1',
    'æœªåˆ°é¤¨'
]

# æ´»å‹•è¨˜éŒ„ (ä»Šæ—¥äº¤æ˜“æ˜ç´°) - æŒä¹…åŒ–åˆ°æª”æ¡ˆ
ACTIVITY_LOG = []
ACTIVITY_LOG_FILE = os.path.join(os.path.dirname(__file__), 'activity_log.json')

def load_activity_log():
    """å¾æª”æ¡ˆè¼‰å…¥æ´»å‹•è¨˜éŒ„"""
    global ACTIVITY_LOG
    try:
        if os.path.exists(ACTIVITY_LOG_FILE):
            with open(ACTIVITY_LOG_FILE, 'r', encoding='utf-8') as f:
                import json
                ACTIVITY_LOG = json.load(f)
                logger.info(f"Loaded {len(ACTIVITY_LOG)} activities from file")
        else:
            ACTIVITY_LOG = []
    except Exception as e:
        logger.error(f"Error loading activity log: {e}")
        ACTIVITY_LOG = []

def save_activity_log():
    """å„²å­˜æ´»å‹•è¨˜éŒ„åˆ°æª”æ¡ˆ"""
    try:
        with open(ACTIVITY_LOG_FILE, 'w', encoding='utf-8') as f:
            import json
            json.dump(ACTIVITY_LOG, f, ensure_ascii=False, indent=2)
        logger.info(f"Saved {len(ACTIVITY_LOG)} activities to file")
    except Exception as e:
        logger.error(f"Error saving activity log: {e}")

def is_valid_date(date_str):
    """æª¢æŸ¥å­—ä¸²æ˜¯å¦ç‚ºæœ‰æ•ˆæ—¥æœŸæ ¼å¼"""
    import re
    # å¸¸è¦‹æ—¥æœŸæ ¼å¼: YYYY-MM-DD, YYYY/MM/DD, MM/DD, DD/MM/YYYY ç­‰
    date_patterns = [
        r'^\d{4}-\d{1,2}-\d{1,2}$',  # 2024-01-30
        r'^\d{4}/\d{1,2}/\d{1,2}$',  # 2024/01/30
        r'^\d{1,2}/\d{1,2}/\d{4}$',  # 01/30/2024
        r'^\d{1,2}/\d{1,2}$',        # 01/30 (æ²’æœ‰å¹´ä»½)
        r'^\d{1,2}-\d{1,2}$',        # 01-30 (æ²’æœ‰å¹´ä»½)
    ]
    for pattern in date_patterns:
        if re.match(pattern, date_str):
            return True
    return False


def add_activity(action, book_data, old_data=None):
    """è¨˜éŒ„æ´»å‹•åˆ°æ—¥èªŒ"""
    global ACTIVITY_LOG
    
    today = datetime.now().strftime('%Y-%m-%d')
    
    # æ¸…ç†éä»Šæ—¥çš„è¨˜éŒ„ (åªä¿ç•™ä»Šæ—¥)
    ACTIVITY_LOG = [a for a in ACTIVITY_LOG if a.get('date', '').startswith(today)]
    
    activity = {
        'id': len(ACTIVITY_LOG) + 1,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'date': datetime.now().strftime('%Y-%m-%d'),
        'time': datetime.now().strftime('%H:%M:%S'),
        'action': action,  # 'add', 'edit', 'delete', 'category_change'
        'book_id': book_data.get('id'),
        'book_title': book_data.get('title', ''),
        'book_author': book_data.get('author', ''),
        'book_category': book_data.get('category', ''),
        'old_category': old_data.get('category', '') if old_data else None,
        'details': {}
    }
    
    # è¨˜éŒ„è®Šæ›´ç´°ç¯€
    if action == 'edit' and old_data:
        changes = []
        for key in ['title', 'author', 'date', 'note', 'category']:
            old_val = old_data.get(key, '')
            new_val = book_data.get(key, '')
            if old_val != new_val:
                changes.append({
                    'field': key,
                    'old': old_val,
                    'new': new_val
                })
        activity['details']['changes'] = changes
    elif action == 'category_change' and old_data:
        activity['details']['old_category'] = old_data.get('category', '')
        activity['details']['new_category'] = book_data.get('category', '')
    
    ACTIVITY_LOG.insert(0, activity)  # æœ€æ–°çš„åœ¨æœ€å‰é¢
    save_activity_log()  # ğŸ’¾ å„²å­˜åˆ°æª”æ¡ˆ
    logger.info(f"Activity logged: {action} - {book_data.get('title', 'Unknown')}")
    
    return activity

def read_all_books():
    """å¾ Excel è®€å–æ‰€æœ‰æ›¸ç± (å«å¿«å–æ©Ÿåˆ¶) - Optimized"""
    global LAST_MTIME, CACHED_BOOKS
    
    try:
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(EXCEL_FILE):
             logger.error(f"Error: æ‰¾ä¸åˆ°æª”æ¡ˆ {EXCEL_FILE}")
             return []

        # Check file modification time
        current_mtime = os.path.getmtime(EXCEL_FILE)
        
        # å¦‚æœæœ‰å¿«å–ä¸”æª”æ¡ˆæ²’è®Šï¼Œç›´æ¥å›å‚³å¿«å–
        if CACHED_BOOKS is not None and current_mtime == LAST_MTIME:
            return CACHED_BOOKS

        print(f"Reading Excel file: {EXCEL_FILE}...")
        books = []
        xls = pd.ExcelFile(EXCEL_FILE)
        book_id = 0
        
        for sheet_name in xls.sheet_names:
            if sheet_name not in CATEGORIES:
                continue
                
            df = pd.read_excel(xls, sheet_name=sheet_name)
            
            # å˜—è©¦æ‰¾å‡ºä½œè€…å’Œæ›¸åæ¬„ä½
            cols = df.columns.tolist()
            has_author = 'ä½œè€…' in cols
            has_title = 'æ›¸å' in cols
            
            # å¦‚æœæ²’æœ‰æ¨™æº–æ¬„ä½ï¼Œå˜—è©¦è™•ç†ç„¡æ¨™é¡Œæˆ–éŒ¯ä½çš„æƒ…æ³
            if not (has_author and has_title):
                 # å˜—è©¦é‡æ–°è®€å– header=None (ç‚ºäº†ç›¸å®¹èˆŠé‚è¼¯ï¼Œé›–ç„¶æœ‰é»æ²’æ•ˆç‡ï¼Œä½†åªé‡å°æ ¼å¼éŒ¯èª¤çš„ sheet)
                 # ç°¡å–®æª¢æŸ¥ï¼šå¦‚æœç¬¬ä¸€åˆ—çœ‹èµ·ä¾†åƒæ¨™é¡Œ
                 is_header_row = False
                 if len(df) > 0:
                     # æª¢æŸ¥ DataFrame çš„ç¬¬ä¸€åˆ—æ˜¯å¦åŒ…å« 'ä½œè€…' æˆ– 'æ›¸å'
                     # é€™è£¡ç°¡åŒ–è™•ç†ï¼šå¦‚æœæ‰¾ä¸åˆ°æ¨™æº–æ¬„ä½ï¼Œå°±å‡è¨­å®ƒæ˜¯ raw dataï¼Œé‡æ–°æ•´ç†
                     # ç‚ºäº†ä¿æŒé«˜æ•ˆï¼Œæˆ‘å€‘ç›´æ¥æ“ä½œ df.values æˆ–é‡è®€
                     # è€ƒæ…®åˆ°é€™ç¨®æƒ…æ³å¾ˆå°‘ï¼Œæˆ‘å€‘å¯ä»¥ç”¨èˆŠçš„ fallback é‚è¼¯ï¼Œæˆ–è€…ç›´æ¥å‡è¨­æ¬„ä½ä½ç½®
                     
                     # é€™è£¡ç‚ºäº†æ•ˆèƒ½ï¼Œæˆ‘å€‘ç›´æ¥ä¾æ¬„ä½ä½ç½®åˆ¤æ–·
                     pass

            # çµ±ä¸€æ¬„ä½åç¨±ä»¥ä¾¿è™•ç†
            target_df = df.copy()
            
            # å®šç¾©æ¬„ä½æ˜ å°„ (å„ªå…ˆä½¿ç”¨åç¨±ï¼Œå¦å‰‡ä½¿ç”¨ä½ç½®)
            col_map = {}
            
            if 'æ›¸å' in cols: col_map['title'] = 'æ›¸å'
            elif len(cols) > 1: col_map['title'] = cols[1]
            else: col_map['title'] = None
            
            if 'ä½œè€…' in cols: col_map['author'] = 'ä½œè€…'
            elif len(cols) > 0: col_map['author'] = cols[0]
            else: col_map['author'] = None
            
            if 'åˆ°æœŸæ—¥' in cols: col_map['date'] = 'åˆ°æœŸæ—¥'
            elif len(cols) > 2: col_map['date'] = cols[2]
            else: col_map['date'] = None
            
            if 'ISBN' in cols: col_map['note'] = 'ISBN'
            elif len(cols) > 3: col_map['note'] = cols[3]
            else: col_map['note'] = None

            if not col_map['title']: continue # ç„¡æ³•è­˜åˆ¥æ›¸åï¼Œè·³é

            # è½‰ç‚ºå­—å…¸åˆ—è¡¨ï¼Œé€Ÿåº¦é å¿«æ–¼ iterrows
            records = target_df.to_dict('records')
            
            for row in records:
                # å–å¾—åŸå§‹å€¼
                r_title = row.get(col_map['title'])
                r_author = row.get(col_map['author'])
                r_date = row.get(col_map['date'])
                r_note = row.get(col_map['note'])
                
                # è™•ç†æ¨™é¡Œ (éæ¿¾æ‰æ¨™é¡Œè¡Œæˆ–ç©ºè¡Œ)
                title = str(r_title).strip() if pd.notna(r_title) else ''
                if not title or title == 'æ›¸å': continue
                
                author = str(r_author).strip() if pd.notna(r_author) else 'æœªåˆ†é¡ä½œè€…'
                if author == 'ä½œè€…': author = 'æœªåˆ†é¡ä½œè€…' # é˜²æ­¢æ¨™é¡Œè¡Œè¢«èª¤è®€
                
                # è™•ç†æ—¥æœŸ
                date = str(r_date).strip() if pd.notna(r_date) else ''
                if ' ' in date: date = date.split(' ')[0]
                if date == 'åˆ°æœŸæ—¥': date = ''
                
                # è™•ç†å‚™è¨»
                note = str(r_note).strip() if pd.notna(r_note) else ''
                if note == 'ISBN': note = ''
                
                # ğŸ”§ è‡ªå‹•ä¿®æ­£ï¼šæª¢æŸ¥ã€Œåˆ°æœŸæ—¥ã€æ¬„ä½æ˜¯å¦è¢«èª¤å¡«ç‚ºå€Ÿé–±äººåç¨±
                # å¦‚æœ date ä¸æ˜¯æ—¥æœŸæ ¼å¼ï¼ˆé YYYY-MM-DD æˆ–é¡ä¼¼æ ¼å¼ï¼‰ï¼Œå°±ç•¶ä½œå‚™è¨»è™•ç†
                if date and not is_valid_date(date):
                    # å¦‚æœ note æ˜¯ç©ºçš„ï¼Œå°±æŠŠéŒ¯èª¤çš„æ—¥æœŸç§»éå»
                    if not note:
                        note = date
                    date = ''  # æ¸…ç©ºæ—¥æœŸæ¬„ä½

                books.append({
                    'id': book_id,
                    'title': title,
                    'author': author if author else 'æœªåˆ†é¡ä½œè€…',
                    'category': sheet_name,
                    'date': date,
                    'note': note
                })
                book_id += 1
                        
        # æ›´æ–°å¿«å–
        CACHED_BOOKS = books
        LAST_MTIME = current_mtime
        logger.info(f"Read {len(books)} books. Updated cache.")
        return books
        
    except Exception as e:
        logger.error(f"è®€å– Excel éŒ¯èª¤: {e}")
        logger.error(traceback.format_exc())
        return CACHED_BOOKS if CACHED_BOOKS is not None else []

def backup_excel():
    """è‡ªå‹•å‚™ä»½ Excel æª”æ¡ˆ"""
    try:
        if not os.path.exists(EXCEL_FILE):
            return
        
        # å»ºç«‹å‚™ä»½è³‡æ–™å¤¾
        backup_dir = os.path.join(os.path.dirname(__file__), 'backups')
        os.makedirs(backup_dir, exist_ok=True)
        
        # å‚™ä»½æª”ååŒ…å«æ™‚é–“æˆ³è¨˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = os.path.join(backup_dir, f'å‚™ä»½_{timestamp}.xlsx')
        
        import shutil
        shutil.copy2(EXCEL_FILE, backup_file)
        logger.info(f"Auto backup created: {backup_file}")
        
        # åªä¿ç•™æœ€è¿‘ 10 ä»½å‚™ä»½
        backups = sorted([f for f in os.listdir(backup_dir) if f.endswith('.xlsx')])
        while len(backups) > 10:
            oldest = backups.pop(0)
            os.remove(os.path.join(backup_dir, oldest))
            logger.info(f"Removed old backup: {oldest}")
            
    except Exception as e:
        logger.error(f"Backup error: {e}")

def save_all_books(books):
    """å°‡æ‰€æœ‰æ›¸ç±å¯«å› Excel (Smart Update)"""
    global CACHED_BOOKS, LAST_MTIME
    try:
        # ğŸ’¾ å„²å­˜å‰è‡ªå‹•å‚™ä»½
        backup_excel()
        
        # æŒ‰åˆ†é¡åˆ†çµ„ (New State)
        categorized = {cat: [] for cat in CATEGORIES}
        for book in books:
            cat = book.get('category', 'æ–°æ›¸-å¾…å€Ÿ')
            if cat in categorized:
                categorized[cat].append(book)
            else:
                categorized['æ–°æ›¸-å¾…å€Ÿ'].append(book)
        
        # åˆ¤æ–·å“ªäº›å·¥ä½œè¡¨æœ‰è®Šæ›´
        changed_sheets = []
        
        # Group Old State (from Cache)
        if CACHED_BOOKS:
            old_categorized = {cat: [] for cat in CATEGORIES}
            for book in CACHED_BOOKS:
                cat = book.get('category', 'æ–°æ›¸-å¾…å€Ÿ')
                if cat in old_categorized:
                    old_categorized[cat].append(book)
                else:
                    old_categorized['æ–°æ›¸-å¾…å€Ÿ'].append(book)
            
            # Compare
            for cat in CATEGORIES:
                if categorized[cat] != old_categorized[cat]:
                    changed_sheets.append(cat)
        else:
            # No cache, assume all changed (or first run)
            changed_sheets = list(CATEGORIES)

        if not changed_sheets and os.path.exists(EXCEL_FILE):
             logger.info("No changes detected. Skip saving.")
             return True

        # è¨­å®šå¯«å…¥æ¨¡å¼
        mode = 'a'
        if_sheet_exists = 'replace'
        
        # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå¿…é ˆç”¨ 'w' æ¨¡å¼å¯«å…¥æ‰€æœ‰å·¥ä½œè¡¨
        if not os.path.exists(EXCEL_FILE):
            mode = 'w'
            if_sheet_exists = None
            changed_sheets = list(CATEGORIES) # Write all
            logger.info("File not found, creating new file (write all sheets).")
        else:
            logger.info(f"Updating sheets: {changed_sheets}")

        # æº–å‚™å¯«å…¥åƒæ•¸
        kwargs = {'engine': 'openpyxl', 'mode': mode}
        if mode == 'a':
            kwargs['if_sheet_exists'] = if_sheet_exists
            
        with pd.ExcelWriter(EXCEL_FILE, **kwargs) as writer:
            for cat in changed_sheets:
                cat_books = categorized[cat]
                if cat_books:
                    df = pd.DataFrame([{
                        'ä½œè€…': b.get('author', 'æœªåˆ†é¡ä½œè€…'),
                        'æ›¸å': b.get('title', ''),
                        'åˆ°æœŸæ—¥': b.get('date', ''),
                        'ISBN': b.get('note', '')
                    } for b in cat_books])
                    df.to_excel(writer, sheet_name=cat, index=False)
                else:
                    # å¯«å…¥ç©ºçš„å·¥ä½œè¡¨ä»¥ä¿ç•™çµæ§‹
                    pd.DataFrame(columns=['ä½œè€…', 'æ›¸å', 'åˆ°æœŸæ—¥', 'ISBN']).to_excel(writer, sheet_name=cat, index=False)
                    
        # æ›´æ–°å¿«å–
        CACHED_BOOKS = books
        # Update mtime to prevent immediate re-read
        if os.path.exists(EXCEL_FILE):
             LAST_MTIME = os.path.getmtime(EXCEL_FILE)
        
        logger.info("Successfully saved books to Excel.")
        return True
    except Exception as e:
        logger.error(f"å¯«å…¥ Excel éŒ¯èª¤: {e}")
        logger.error(traceback.format_exc())
        return False

# API è·¯ç”±

@app.route('/api/books', methods=['GET'])
def get_books():
    """å–å¾—æ‰€æœ‰æ›¸ç±"""
    books = read_all_books()
    return jsonify(books)

@app.route('/api/books', methods=['POST'])
def add_book():
    """æ–°å¢æ›¸ç±"""
    try:
        data = request.json
        logger.info(f"Adding new book: {data.get('title', 'Unknown')}")
        
        current_books = read_all_books()
        # Create a copy to avoid modifying cache before save success
        books = list(current_books)
        
        new_id = max([b['id'] for b in books], default=-1) + 1
        new_book = {
            'id': new_id,
            'title': data.get('title', ''),
            'author': data.get('author', 'æœªåˆ†é¡ä½œè€…'),
            'category': data.get('category', 'æ–°æ›¸-å¾…å€Ÿ'),
            'date': data.get('date', ''),
            'note': data.get('note', '')
        }
        
        # Insert at the beginning
        books.insert(0, new_book)
        
        if save_all_books(books):
            # è¨˜éŒ„æ´»å‹•
            add_activity('add', new_book)
            logger.info(f"Book added successfully: ID {new_id}")
            return jsonify(new_book), 201
        else:
            logger.error("Failed to save book to Excel")
            return jsonify({'error': 'å„²å­˜å¤±æ•—'}), 500
            
    except Exception as e:
        logger.error(f"Error in add_book: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

@app.route('/api/books/<int:book_id>', methods=['PUT'])
def update_book(book_id):
    """æ›´æ–°æ›¸ç±"""
    data = request.json
    books = read_all_books()
    old_book = None
    updated_book = None
    
    for i, book in enumerate(books):
        if book['id'] == book_id:
            old_book = book.copy()  # ä¿å­˜èˆŠè³‡æ–™
            books[i] = {
                'id': book_id,
                'title': data.get('title', book['title']),
                'author': data.get('author', book['author']),
                'category': data.get('category', book['category']),
                'date': data.get('date', book.get('date', '')),
                'note': data.get('note', book.get('note', ''))
            }
            updated_book = books[i]
            break
    
    if save_all_books(books):
        # åˆ¤æ–·ç·¨è¼¯é¡å‹
        if old_book and updated_book:
            if old_book.get('category') != updated_book.get('category'):
                add_activity('category_change', updated_book, old_book)
            else:
                add_activity('edit', updated_book, old_book)
        return jsonify(updated_book)
    else:
        return jsonify({'error': 'å„²å­˜å¤±æ•—'}), 500

@app.route('/api/books/<int:book_id>', methods=['DELETE'])
def delete_book(book_id):
    """åˆªé™¤æ›¸ç±"""
    books = read_all_books()
    deleted_book = None
    
    # å…ˆæ‰¾åˆ°è¦åˆªé™¤çš„æ›¸
    for b in books:
        if b['id'] == book_id:
            deleted_book = b.copy()
            break
    
    books = [b for b in books if b['id'] != book_id]
    
    if save_all_books(books):
        # è¨˜éŒ„åˆªé™¤æ´»å‹•
        if deleted_book:
            add_activity('delete', deleted_book)
        return jsonify({'success': True})
    else:
        return jsonify({'error': 'å„²å­˜å¤±æ•—'}), 500

@app.route('/api/stats', methods=['GET'])
def get_stats():
    """å–å¾—çµ±è¨ˆè³‡æ–™"""
    books = read_all_books()
    
    # ä½œè€…çµ±è¨ˆ
    authors = {}
    for book in books:
        author = book.get('author', 'æœªåˆ†é¡ä½œè€…')
        if author and author != 'æœªåˆ†é¡ä½œè€…':
            if author not in authors:
                authors[author] = []
            authors[author].append(book['title'])
    
    # åˆ†é¡çµ±è¨ˆ
    category_stats = {}
    for cat in CATEGORIES:
        category_stats[cat] = len([b for b in books if b.get('category') == cat])
    
    return jsonify({
        'total_books': len(books),
        'total_authors': len(authors),
        'category_stats': category_stats,
        'authors': authors
    })

@app.route('/api/export', methods=['GET'])
def export_books():
    """åŒ¯å‡º Excel æª”æ¡ˆ"""
    try:
        if not os.path.exists(EXCEL_FILE):
             return jsonify({'error': 'æ‰¾ä¸åˆ°åŸå§‹æª”æ¡ˆ'}), 404
             
        # ç¢ºä¿ Excel æª”æ¡ˆå­˜åœ¨ä¸”æœ€æ–° (è‹¥æ˜¯è¨˜æ†¶é«”æœ‰æ›´æ–°ä½† save å¤±æ•—çš„æƒ…æ³... ä½†é€šå¸¸ save æœƒæˆåŠŸ)
        # é€™è£¡ç›´æ¥å‚³é€æª”æ¡ˆå³å¯ï¼Œå› ç‚ºæ‰€æœ‰ä¿®æ”¹éƒ½æœƒç«‹å³å¯«å…¥æª”æ¡ˆ
            
        return send_file(
            EXCEL_FILE,
            as_attachment=True,
            download_name=f'library_books_{datetime.now().strftime("%Y%m%d")}.xlsx',
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
    except Exception as e:
        logger.error(f"Export error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/debug/reload', methods=['POST'])
def force_reload():
    """å¼·åˆ¶é‡è®€ Excel (æ¸…é™¤å¿«å–)"""
    global CACHED_BOOKS, LAST_MTIME
    CACHED_BOOKS = None
    LAST_MTIME = 0
    books = read_all_books()
    return jsonify({'message': 'Cache cleared', 'count': len(books)})

@app.route('/api/activities', methods=['GET'])
def get_activities():
    """å–å¾—ä»Šæ—¥æ´»å‹•è¨˜éŒ„"""
    global ACTIVITY_LOG
    
    today = datetime.now().strftime('%Y-%m-%d')
    # åªå›å‚³ä»Šæ—¥çš„è¨˜éŒ„
    today_activities = [a for a in ACTIVITY_LOG if a.get('date', '').startswith(today)]
    
    # çµ±è¨ˆè³‡è¨Š
    stats = {
        'total': len(today_activities),
        'adds': len([a for a in today_activities if a['action'] == 'add']),
        'edits': len([a for a in today_activities if a['action'] == 'edit']),
        'deletes': len([a for a in today_activities if a['action'] == 'delete']),
        'category_changes': len([a for a in today_activities if a['action'] == 'category_change']),
        'date': today
    }
    
    return jsonify({
        'activities': today_activities,
        'stats': stats
    })

@app.route('/api/activities', methods=['DELETE'])
def clear_activities():
    """æ¸…é™¤æ‰€æœ‰æ´»å‹•è¨˜éŒ„"""
    global ACTIVITY_LOG
    ACTIVITY_LOG = []
    save_activity_log()  # ğŸ’¾ åŒæ™‚æ¸…ç©ºæª”æ¡ˆ
    return jsonify({'success': True, 'message': 'æ´»å‹•è¨˜éŒ„å·²æ¸…é™¤'})

if __name__ == '__main__':
    print("=" * 50)
    print("ğŸ“š åœ–æ›¸é¤¨å€Ÿæ›¸ç®¡ç†ç³»çµ± - API æœå‹™")
    print("=" * 50)
    print(f"Excel æª”æ¡ˆ: {EXCEL_FILE}")
    print(f"API ç¶²å€: http://localhost:5001")
    print("=" * 50)
    
    # ğŸ“‚ å•Ÿå‹•æ™‚è¼‰å…¥æ´»å‹•è¨˜éŒ„
    load_activity_log()
    print(f"å·²è¼‰å…¥ {len(ACTIVITY_LOG)} ç­†ä»Šæ—¥æ´»å‹•è¨˜éŒ„")
    
    app.run(host='0.0.0.0', debug=True, port=5001, use_reloader=False) # Disable reloader to prevent double loops in some envs
