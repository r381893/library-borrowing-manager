"""
åœ–æ›¸é¤¨å€Ÿæ›¸ç®¡ç†ç³»çµ± - Python å¾Œç«¯ API
ç›´æ¥è®€å¯« Excel æª”æ¡ˆï¼Œæä¾› RESTful API çµ¦å‰ç«¯ä½¿ç”¨
"""

from flask import Flask, jsonify, request, send_file
from flask_cors import CORS
import pandas as pd
import os
from datetime import datetime
import logging
import traceback

# è¨­å®š Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("server.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # å…è¨±è·¨åŸŸè«‹æ±‚

# Excel æª”æ¡ˆè·¯å¾‘
EXCEL_FILE = os.path.join(os.path.dirname(__file__), 'åœ–æ›¸é¤¨å€Ÿæ›¸æ¸…å–®.xlsx')
LAST_MTIME = 0
CACHED_BOOKS = None

# åˆ†é¡å°æ‡‰çš„å·¥ä½œè¡¨åç¨±
CATEGORIES = [
    'æ–°æ›¸-å¾…å€Ÿ',
    'å¾…å€Ÿ',
    'ä¸èƒ½å€Ÿ',
    'é£Ÿè­œ',
    'é æ•¸å¤ªå¤š',
    'å·²çœ‹-3447æœ¬',
    'å·²çœ‹-1',
    'æœªåˆ°é¤¨'
]

def read_all_books():
    """å¾ Excel è®€å–æ‰€æœ‰æ›¸ç± (å«å¿«å–æ©Ÿåˆ¶) - Optimized"""
    global LAST_MTIME, CACHED_BOOKS
    
    try:
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(EXCEL_FILE):
             logger.error(f"Error: æ‰¾ä¸åˆ°æª”æ¡ˆ {EXCEL_FILE}")
             return []

        # Check file modification time
        current_mtime = os.path.getmtime(EXCEL_FILE)
        
        # å¦‚æœæœ‰å¿«å–ä¸”æª”æ¡ˆæ²’è®Šï¼Œç›´æ¥å›å‚³å¿«å–
        if CACHED_BOOKS is not None and current_mtime == LAST_MTIME:
            return CACHED_BOOKS

        print(f"Reading Excel file: {EXCEL_FILE}...")
        books = []
        xls = pd.ExcelFile(EXCEL_FILE)
        book_id = 0
        
        for sheet_name in xls.sheet_names:
            if sheet_name not in CATEGORIES:
                continue
                
            df = pd.read_excel(xls, sheet_name=sheet_name)
            
            # å˜—è©¦æ‰¾å‡ºä½œè€…å’Œæ›¸åæ¬„ä½
            cols = df.columns.tolist()
            has_author = 'ä½œè€…' in cols
            has_title = 'æ›¸å' in cols
            
            # å¦‚æœæ²’æœ‰æ¨™æº–æ¬„ä½ï¼Œå˜—è©¦è™•ç†ç„¡æ¨™é¡Œæˆ–éŒ¯ä½çš„æƒ…æ³
            if not (has_author and has_title):
                 # å˜—è©¦é‡æ–°è®€å– header=None (ç‚ºäº†ç›¸å®¹èˆŠé‚è¼¯ï¼Œé›–ç„¶æœ‰é»æ²’æ•ˆç‡ï¼Œä½†åªé‡å°æ ¼å¼éŒ¯èª¤çš„ sheet)
                 # ç°¡å–®æª¢æŸ¥ï¼šå¦‚æœç¬¬ä¸€åˆ—çœ‹èµ·ä¾†åƒæ¨™é¡Œ
                 is_header_row = False
                 if len(df) > 0:
                     # æª¢æŸ¥ DataFrame çš„ç¬¬ä¸€åˆ—æ˜¯å¦åŒ…å« 'ä½œè€…' æˆ– 'æ›¸å'
                     # é€™è£¡ç°¡åŒ–è™•ç†ï¼šå¦‚æœæ‰¾ä¸åˆ°æ¨™æº–æ¬„ä½ï¼Œå°±å‡è¨­å®ƒæ˜¯ raw dataï¼Œé‡æ–°æ•´ç†
                     # ç‚ºäº†ä¿æŒé«˜æ•ˆï¼Œæˆ‘å€‘ç›´æ¥æ“ä½œ df.values æˆ–é‡è®€
                     # è€ƒæ…®åˆ°é€™ç¨®æƒ…æ³å¾ˆå°‘ï¼Œæˆ‘å€‘å¯ä»¥ç”¨èˆŠçš„ fallback é‚è¼¯ï¼Œæˆ–è€…ç›´æ¥å‡è¨­æ¬„ä½ä½ç½®
                     
                     # é€™è£¡ç‚ºäº†æ•ˆèƒ½ï¼Œæˆ‘å€‘ç›´æ¥ä¾æ¬„ä½ä½ç½®åˆ¤æ–·
                     pass

            # çµ±ä¸€æ¬„ä½åç¨±ä»¥ä¾¿è™•ç†
            target_df = df.copy()
            
            # å®šç¾©æ¬„ä½æ˜ å°„ (å„ªå…ˆä½¿ç”¨åç¨±ï¼Œå¦å‰‡ä½¿ç”¨ä½ç½®)
            col_map = {}
            
            if 'æ›¸å' in cols: col_map['title'] = 'æ›¸å'
            elif len(cols) > 1: col_map['title'] = cols[1]
            else: col_map['title'] = None
            
            if 'ä½œè€…' in cols: col_map['author'] = 'ä½œè€…'
            elif len(cols) > 0: col_map['author'] = cols[0]
            else: col_map['author'] = None
            
            if 'åˆ°æœŸæ—¥' in cols: col_map['date'] = 'åˆ°æœŸæ—¥'
            elif len(cols) > 2: col_map['date'] = cols[2]
            else: col_map['date'] = None
            
            if 'ISBN' in cols: col_map['note'] = 'ISBN'
            elif len(cols) > 3: col_map['note'] = cols[3]
            else: col_map['note'] = None

            if not col_map['title']: continue # ç„¡æ³•è­˜åˆ¥æ›¸åï¼Œè·³é

            # è½‰ç‚ºå­—å…¸åˆ—è¡¨ï¼Œé€Ÿåº¦é å¿«æ–¼ iterrows
            records = target_df.to_dict('records')
            
            for row in records:
                # å–å¾—åŸå§‹å€¼
                r_title = row.get(col_map['title'])
                r_author = row.get(col_map['author'])
                r_date = row.get(col_map['date'])
                r_note = row.get(col_map['note'])
                
                # è™•ç†æ¨™é¡Œ (éæ¿¾æ‰æ¨™é¡Œè¡Œæˆ–ç©ºè¡Œ)
                title = str(r_title).strip() if pd.notna(r_title) else ''
                if not title or title == 'æ›¸å': continue
                
                author = str(r_author).strip() if pd.notna(r_author) else 'æœªåˆ†é¡ä½œè€…'
                if author == 'ä½œè€…': author = 'æœªåˆ†é¡ä½œè€…' # é˜²æ­¢æ¨™é¡Œè¡Œè¢«èª¤è®€
                
                # è™•ç†æ—¥æœŸ
                date = str(r_date).strip() if pd.notna(r_date) else ''
                if ' ' in date: date = date.split(' ')[0]
                if date == 'åˆ°æœŸæ—¥': date = ''
                
                # è™•ç†å‚™è¨»
                note = str(r_note).strip() if pd.notna(r_note) else ''
                if note == 'ISBN': note = ''

                books.append({
                    'id': book_id,
                    'title': title,
                    'author': author if author else 'æœªåˆ†é¡ä½œè€…',
                    'category': sheet_name,
                    'date': date,
                    'note': note
                })
                book_id += 1
                        
        # æ›´æ–°å¿«å–
        CACHED_BOOKS = books
        LAST_MTIME = current_mtime
        logger.info(f"Read {len(books)} books. Updated cache.")
        return books
        
    except Exception as e:
        logger.error(f"è®€å– Excel éŒ¯èª¤: {e}")
        logger.error(traceback.format_exc())
        return CACHED_BOOKS if CACHED_BOOKS is not None else []

def save_all_books(books):
    """å°‡æ‰€æœ‰æ›¸ç±å¯«å› Excel (Smart Update)"""
    global CACHED_BOOKS, LAST_MTIME
    try:
        # æŒ‰åˆ†é¡åˆ†çµ„ (New State)
        categorized = {cat: [] for cat in CATEGORIES}
        for book in books:
            cat = book.get('category', 'æ–°æ›¸-å¾…å€Ÿ')
            if cat in categorized:
                categorized[cat].append(book)
            else:
                categorized['æ–°æ›¸-å¾…å€Ÿ'].append(book)
        
        # åˆ¤æ–·å“ªäº›å·¥ä½œè¡¨æœ‰è®Šæ›´
        changed_sheets = []
        
        # Group Old State (from Cache)
        if CACHED_BOOKS:
            old_categorized = {cat: [] for cat in CATEGORIES}
            for book in CACHED_BOOKS:
                cat = book.get('category', 'æ–°æ›¸-å¾…å€Ÿ')
                if cat in old_categorized:
                    old_categorized[cat].append(book)
                else:
                    old_categorized['æ–°æ›¸-å¾…å€Ÿ'].append(book)
            
            # Compare
            for cat in CATEGORIES:
                if categorized[cat] != old_categorized[cat]:
                    changed_sheets.append(cat)
        else:
            # No cache, assume all changed (or first run)
            changed_sheets = list(CATEGORIES)

        if not changed_sheets and os.path.exists(EXCEL_FILE):
             logger.info("No changes detected. Skip saving.")
             return True

        # è¨­å®šå¯«å…¥æ¨¡å¼
        mode = 'a'
        if_sheet_exists = 'replace'
        
        # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå¿…é ˆç”¨ 'w' æ¨¡å¼å¯«å…¥æ‰€æœ‰å·¥ä½œè¡¨
        if not os.path.exists(EXCEL_FILE):
            mode = 'w'
            if_sheet_exists = None
            changed_sheets = list(CATEGORIES) # Write all
            logger.info("File not found, creating new file (write all sheets).")
        else:
            logger.info(f"Updating sheets: {changed_sheets}")

        # æº–å‚™å¯«å…¥åƒæ•¸
        kwargs = {'engine': 'openpyxl', 'mode': mode}
        if mode == 'a':
            kwargs['if_sheet_exists'] = if_sheet_exists
            
        with pd.ExcelWriter(EXCEL_FILE, **kwargs) as writer:
            for cat in changed_sheets:
                cat_books = categorized[cat]
                if cat_books:
                    df = pd.DataFrame([{
                        'ä½œè€…': b.get('author', 'æœªåˆ†é¡ä½œè€…'),
                        'æ›¸å': b.get('title', ''),
                        'åˆ°æœŸæ—¥': b.get('date', ''),
                        'ISBN': b.get('note', '')
                    } for b in cat_books])
                    df.to_excel(writer, sheet_name=cat, index=False)
                else:
                    # å¯«å…¥ç©ºçš„å·¥ä½œè¡¨ä»¥ä¿ç•™çµæ§‹
                    pd.DataFrame(columns=['ä½œè€…', 'æ›¸å', 'åˆ°æœŸæ—¥', 'ISBN']).to_excel(writer, sheet_name=cat, index=False)
                    
        # æ›´æ–°å¿«å–
        CACHED_BOOKS = books
        # Update mtime to prevent immediate re-read
        if os.path.exists(EXCEL_FILE):
             LAST_MTIME = os.path.getmtime(EXCEL_FILE)
        
        logger.info("Successfully saved books to Excel.")
        return True
    except Exception as e:
        logger.error(f"å¯«å…¥ Excel éŒ¯èª¤: {e}")
        logger.error(traceback.format_exc())
        return False

# API è·¯ç”±

@app.route('/api/books', methods=['GET'])
def get_books():
    """å–å¾—æ‰€æœ‰æ›¸ç±"""
    books = read_all_books()
    return jsonify(books)

@app.route('/api/books', methods=['POST'])
def add_book():
    """æ–°å¢æ›¸ç±"""
    try:
        data = request.json
        logger.info(f"Adding new book: {data.get('title', 'Unknown')}")
        
        current_books = read_all_books()
        # Create a copy to avoid modifying cache before save success
        books = list(current_books)
        
        new_id = max([b['id'] for b in books], default=-1) + 1
        new_book = {
            'id': new_id,
            'title': data.get('title', ''),
            'author': data.get('author', 'æœªåˆ†é¡ä½œè€…'),
            'category': data.get('category', 'æ–°æ›¸-å¾…å€Ÿ'),
            'date': data.get('date', ''),
            'note': data.get('note', '')
        }
        
        # Insert at the beginning
        books.insert(0, new_book)
        
        if save_all_books(books):
            logger.info(f"Book added successfully: ID {new_id}")
            return jsonify(new_book), 201
        else:
            logger.error("Failed to save book to Excel")
            return jsonify({'error': 'å„²å­˜å¤±æ•—'}), 500
            
    except Exception as e:
        logger.error(f"Error in add_book: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

@app.route('/api/books/<int:book_id>', methods=['PUT'])
def update_book(book_id):
    """æ›´æ–°æ›¸ç±"""
    data = request.json
    books = read_all_books()
    
    for i, book in enumerate(books):
        if book['id'] == book_id:
            books[i] = {
                'id': book_id,
                'title': data.get('title', book['title']),
                'author': data.get('author', book['author']),
                'category': data.get('category', book['category']),
                'date': data.get('date', book.get('date', '')),
                'note': data.get('note', book.get('note', ''))
            }
            break
    
    if save_all_books(books):
        return jsonify(books[i])
    else:
        return jsonify({'error': 'å„²å­˜å¤±æ•—'}), 500

@app.route('/api/books/<int:book_id>', methods=['DELETE'])
def delete_book(book_id):
    """åˆªé™¤æ›¸ç±"""
    books = read_all_books()
    books = [b for b in books if b['id'] != book_id]
    
    if save_all_books(books):
        return jsonify({'success': True})
    else:
        return jsonify({'error': 'å„²å­˜å¤±æ•—'}), 500

@app.route('/api/stats', methods=['GET'])
def get_stats():
    """å–å¾—çµ±è¨ˆè³‡æ–™"""
    books = read_all_books()
    
    # ä½œè€…çµ±è¨ˆ
    authors = {}
    for book in books:
        author = book.get('author', 'æœªåˆ†é¡ä½œè€…')
        if author and author != 'æœªåˆ†é¡ä½œè€…':
            if author not in authors:
                authors[author] = []
            authors[author].append(book['title'])
    
    # åˆ†é¡çµ±è¨ˆ
    category_stats = {}
    for cat in CATEGORIES:
        category_stats[cat] = len([b for b in books if b.get('category') == cat])
    
    return jsonify({
        'total_books': len(books),
        'total_authors': len(authors),
        'category_stats': category_stats,
        'authors': authors
    })

@app.route('/api/export', methods=['GET'])
def export_books():
    """åŒ¯å‡º Excel æª”æ¡ˆ"""
    try:
        if not os.path.exists(EXCEL_FILE):
             return jsonify({'error': 'æ‰¾ä¸åˆ°åŸå§‹æª”æ¡ˆ'}), 404
             
        # ç¢ºä¿ Excel æª”æ¡ˆå­˜åœ¨ä¸”æœ€æ–° (è‹¥æ˜¯è¨˜æ†¶é«”æœ‰æ›´æ–°ä½† save å¤±æ•—çš„æƒ…æ³... ä½†é€šå¸¸ save æœƒæˆåŠŸ)
        # é€™è£¡ç›´æ¥å‚³é€æª”æ¡ˆå³å¯ï¼Œå› ç‚ºæ‰€æœ‰ä¿®æ”¹éƒ½æœƒç«‹å³å¯«å…¥æª”æ¡ˆ
            
        return send_file(
            EXCEL_FILE,
            as_attachment=True,
            download_name=f'library_books_{datetime.now().strftime("%Y%m%d")}.xlsx',
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
    except Exception as e:
        logger.error(f"Export error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/debug/reload', methods=['POST'])
def force_reload():
    """å¼·åˆ¶é‡è®€ Excel (æ¸…é™¤å¿«å–)"""
    global CACHED_BOOKS, LAST_MTIME
    CACHED_BOOKS = None
    LAST_MTIME = 0
    books = read_all_books()
    return jsonify({'message': 'Cache cleared', 'count': len(books)})

if __name__ == '__main__':
    print("=" * 50)
    print("ğŸ“š åœ–æ›¸é¤¨å€Ÿæ›¸ç®¡ç†ç³»çµ± - API æœå‹™")
    print("=" * 50)
    print(f"Excel æª”æ¡ˆ: {EXCEL_FILE}")
    print(f"API ç¶²å€: http://localhost:5000")
    print("=" * 50)
    app.run(host='0.0.0.0', debug=True, port=5000, use_reloader=False) # Disable reloader to prevent double loops in some envs
